[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "Hi! I‚Äôm Jo√£o Gabriel, a data scientist with a strong passion for problem-solving, currently located in S√£o Paulo, Brazil.\nI have a deep interest in programming, particularly in data science, machine learning, and artificial intelligence, as well as in developing interactive applications for data visualization. My strong background in mathematics and statistics enables me to ensure the consistency and optimization of my code while deepening my understanding of concepts that arise in my daily work and projects.\nCurrently, I work as a data scientist, primarily using Python, R, SQL, Julia, and Markdown for data manipulation and the creation of analytical reports. In the field of machine learning, I utilize libraries such as scikit-learn (Python) and tidymodels (R) for model building and evaluation. I also develop applications with Shiny, focusing on data visualization through interactive dashboards and platforms for monitoring and tracking the performance of models in production.\nBesides that, I‚Äôm also an amateur jazz pianist and guitarist.\nHere, I‚Äôll share some insights into my personal projects, ranging from math and machine learning to automation, data science, and visualization.",
    "crumbs": [
      "About me"
    ]
  },
  {
    "objectID": "index.html#academic-education",
    "href": "index.html#academic-education",
    "title": "About me",
    "section": "Academic education",
    "text": "Academic education\nI have a bachelor‚Äôs degree in mathematics from State University of S√£o Paulo (UNESP) and a master‚Äôs degree in mathematics from State University of Campinas (UNICAMP) in the area of Algebra and Number Theory. Some of my articles are listed below, as well as my master‚Äôs thesis:\n\n\nDiagonal 2 √ó 2 Space-Time Block Codes with Good Normalized Densities \nMaster‚Äôs thesis \n\n\nFor further details, check my Lattes CV !",
    "crumbs": [
      "About me"
    ]
  },
  {
    "objectID": "index.html#some-of-my-projects",
    "href": "index.html#some-of-my-projects",
    "title": "About me",
    "section": "Some of my projects",
    "text": "Some of my projects",
    "crumbs": [
      "About me"
    ]
  },
  {
    "objectID": "articles/survival_analysis_with_tidymodels/article.html",
    "href": "articles/survival_analysis_with_tidymodels/article.html",
    "title": "Survival analysis with tidymodels",
    "section": "",
    "text": "Survival analysis is an important field in statistics and modeling that studies the expected duration until the occurrence of one or more specified events. Although it may seem like a regular time-to-event analysis, survival methods consider observations where the event of interest may not have occurred yet, i.e, it considers censored data.\nThe key concepts in survival analysis are:\n\nevent: The occurrence of the outcome of interest, such as death or relapse of a disease.\ntime: Interval from the start of an observation period until one of the following: occurrence of an event, end of the study, or loss of contact or withdrawal from the study.\ncensoring: Occurs when a subject does not experience the event during the observation period. This implies that a censored subject may or may not experience the event after the end of the observation period.\n\nThere are two main statistics that describe survival data: survival probability and hazard probability.\nSurvival probability denotes the likelihood that an subject will not experience the event of interest from a given time origin up to a future time \\(t\\) (survive). On the other hand, hazard probability represents the likelihood that an individual under observation at time \\(t\\) will experience the event at that moment.\nUnlike survival probability, which emphasizes the absence of an event, the hazard probability focuses on the occurrence of the event. Essentially, the hazard describes the current rate of events, while survival reflects the cumulative non-occurrence over time.\nFor a more detailed approach to the basics of survival analysis, see (Clark et al. 2003)."
  },
  {
    "objectID": "articles/survival_analysis_with_tidymodels/article.html#introducing-survival-analysis",
    "href": "articles/survival_analysis_with_tidymodels/article.html#introducing-survival-analysis",
    "title": "Survival analysis with tidymodels",
    "section": "",
    "text": "Survival analysis is an important field in statistics and modeling that studies the expected duration until the occurrence of one or more specified events. Although it may seem like a regular time-to-event analysis, survival methods consider observations where the event of interest may not have occurred yet, i.e, it considers censored data.\nThe key concepts in survival analysis are:\n\nevent: The occurrence of the outcome of interest, such as death or relapse of a disease.\ntime: Interval from the start of an observation period until one of the following: occurrence of an event, end of the study, or loss of contact or withdrawal from the study.\ncensoring: Occurs when a subject does not experience the event during the observation period. This implies that a censored subject may or may not experience the event after the end of the observation period.\n\nThere are two main statistics that describe survival data: survival probability and hazard probability.\nSurvival probability denotes the likelihood that an subject will not experience the event of interest from a given time origin up to a future time \\(t\\) (survive). On the other hand, hazard probability represents the likelihood that an individual under observation at time \\(t\\) will experience the event at that moment.\nUnlike survival probability, which emphasizes the absence of an event, the hazard probability focuses on the occurrence of the event. Essentially, the hazard describes the current rate of events, while survival reflects the cumulative non-occurrence over time.\nFor a more detailed approach to the basics of survival analysis, see (Clark et al. 2003)."
  },
  {
    "objectID": "articles/survival_analysis_with_tidymodels/article.html#the-data",
    "href": "articles/survival_analysis_with_tidymodels/article.html#the-data",
    "title": "Survival analysis with tidymodels",
    "section": "The data",
    "text": "The data\nSurvival data are time-to-event data with distinct start time and end time. In this article, we‚Äôll use the building complaints dataset of the modeldatatoo package. You can see more information about the data set here.\nThis is the same dataset used in the survival case study 1 and survival metrics2 tidymodels articles. It includes information about complaints received by the Department of Buildings in New York city, such as type of the complaint, the date that it was entered in their records, the date it was dispositioned, the location and the neighborhood of the building, and the status of the complaint."
  },
  {
    "objectID": "articles/survival_analysis_with_tidymodels/article.html#exploratory-data-analysis",
    "href": "articles/survival_analysis_with_tidymodels/article.html#exploratory-data-analysis",
    "title": "Survival analysis with tidymodels",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nFirst, we‚Äôll have a look on the data structure and in it‚Äôs firsts rows:\n\ndata &lt;- modeldatatoo::building_complaints\n\n\n\nRows: 4,234\nColumns: 11\n$ days_to_disposition &lt;dbl&gt; 72, 1, 41, 45, 16, 62, 56, 11, 35, 38, 39, 106, 1,‚Ä¶\n$ status              &lt;chr&gt; \"ACTIVE\", \"ACTIVE\", \"ACTIVE\", \"ACTIVE\", \"ACTIVE\", ‚Ä¶\n$ year_entered        &lt;fct&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 20‚Ä¶\n$ latitude            &lt;dbl&gt; 40.66173, 40.57668, 40.73242, 40.68245, 40.63156, ‚Ä¶\n$ longitude           &lt;dbl&gt; -73.98297, -74.00453, -73.87630, -73.79367, -73.99‚Ä¶\n$ borough             &lt;fct&gt; Brooklyn, Brooklyn, Queens, Queens, Brooklyn, Quee‚Ä¶\n$ special_district    &lt;fct&gt; None, None, None, None, None, None, None, None, No‚Ä¶\n$ unit                &lt;fct&gt; Q-L, Q-L, SPOPS, Q-L, BKLYN, Q-L, Q-L, SPOPS, Q-L,‚Ä¶\n$ community_board     &lt;fct&gt; 307, 313, 404, 412, 312, 406, 306, 306, 409, 404, ‚Ä¶\n$ complaint_category  &lt;fct&gt; 45, 45, 49, 45, 31, 45, 45, 49, 45, 45, 45, 4A, 31‚Ä¶\n$ complaint_priority  &lt;fct&gt; B, B, C, B, C, B, B, C, B, B, B, B, C, C, B, B, B,‚Ä¶"
  },
  {
    "objectID": "articles/survival_analysis_with_tidymodels/article.html#metrics",
    "href": "articles/survival_analysis_with_tidymodels/article.html#metrics",
    "title": "Survival analysis with tidymodels",
    "section": "Metrics",
    "text": "Metrics\nsahsahshas"
  },
  {
    "objectID": "articles/survival_analysis_with_tidymodels/article.html#footnotes",
    "href": "articles/survival_analysis_with_tidymodels/article.html#footnotes",
    "title": "Survival analysis with tidymodels",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHow long until building complaints are dispositioned? A survival analysis case study‚Ü©Ô∏é\nDynamic Performance Metrics for Event Time Data‚Ü©Ô∏é"
  },
  {
    "objectID": "articles/price_tracking_with_py_selenium/article.html",
    "href": "articles/price_tracking_with_py_selenium/article.html",
    "title": "Price tracking with Selenium",
    "section": "",
    "text": "Selenium is an open-source framework that enables you to write tests or scripts to control a web browser. It supports multiple programming languages, including Python. With Selenium, you can simulate user interactions like clicking buttons, entering text, and navigating through pages.\nIn Python, Selenium offers an powerful API that‚Äôs great for automating tasks. In this article, we‚Äôll guide you through the setup process and demonstrate how to use Selenium to track prices automatically!\n\nThe jupyter notebook of this article can be found here.",
    "crumbs": [
      "Articles",
      "Automation",
      "Price tracking with Selenium"
    ]
  },
  {
    "objectID": "articles/price_tracking_with_py_selenium/article.html#what-is-selenium",
    "href": "articles/price_tracking_with_py_selenium/article.html#what-is-selenium",
    "title": "Price tracking with Selenium",
    "section": "",
    "text": "Selenium is an open-source framework that enables you to write tests or scripts to control a web browser. It supports multiple programming languages, including Python. With Selenium, you can simulate user interactions like clicking buttons, entering text, and navigating through pages.\nIn Python, Selenium offers an powerful API that‚Äôs great for automating tasks. In this article, we‚Äôll guide you through the setup process and demonstrate how to use Selenium to track prices automatically!\n\nThe jupyter notebook of this article can be found here.",
    "crumbs": [
      "Articles",
      "Automation",
      "Price tracking with Selenium"
    ]
  },
  {
    "objectID": "articles/price_tracking_with_py_selenium/article.html#the-basics",
    "href": "articles/price_tracking_with_py_selenium/article.html#the-basics",
    "title": "Price tracking with Selenium",
    "section": "The basics",
    "text": "The basics\nSelenium is made up of several components, but we‚Äôll focus on the Selenium WebDriver, which enables you to control the browser. To get started with Selenium, you‚Äôll need a WebDriver for the browser you wish to automate. In this guide, we‚Äôll be using the Chrome WebDriver.\nNormally, setting up Selenium involves downloading the ChromeDriver manually and configuring it on your system, but by using the webdriver-manager library, this process is automated: it downloads the correct version of ChromeDriver based on the version of Chrome installed on your machine.\nNow, let‚Äôs see this in action by creating a Chrome WebDriver instance, navigating to Google, print the title page and then closing the driver.\nfrom selenium import webdriver\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.chrome.service import Service\n\ndef create_driver(url: str = None) -&gt; webdriver.Chrome:\n    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n    \n    if url:\n        driver.get(url)\n    \n    return driver\n\ndriver = create_driver(url=\"https://www.google.com\")\nprint(driver.title)\n\ndriver.quit()",
    "crumbs": [
      "Articles",
      "Automation",
      "Price tracking with Selenium"
    ]
  },
  {
    "objectID": "articles/price_tracking_with_py_selenium/article.html#finding-elements",
    "href": "articles/price_tracking_with_py_selenium/article.html#finding-elements",
    "title": "Price tracking with Selenium",
    "section": "Finding elements",
    "text": "Finding elements\nBefore we can perform any action, we need to locate the elements on the page. To do this, simply right-click on the desired element and select ‚ÄúInspect‚Äù to open devtools, where we can find its identifier.\nThe identifier can be the element‚Äôs ID, class, or XPath. Once you have it, you can use methods to retrieve the element.\nHowever, sometimes elements may not be clickable due to various factors, such as the page still loading, which can lead to errors in automation. To adress this issue, we can use the WebDriverWait module, which allows us to pause the execution until a certain condition is met, such as an element becoming clickable.\nTo wait for an element to be clickable, we can use the following code:\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nwait = WebDriverWait(driver, wait_time)\nwait.until(EC.element_to_be_clickable((by, element)))\nHere, by is the method we‚Äôre using to locate the element (like By.ID or By.XPATH), and element is the identifier of the target element. Additionally, the Expected Conditions (EC) module provides useful conditions to help us manage waiting for specific states of web elements.",
    "crumbs": [
      "Articles",
      "Automation",
      "Price tracking with Selenium"
    ]
  },
  {
    "objectID": "articles/price_tracking_with_py_selenium/article.html#moving-to-and-interacting-with-elements",
    "href": "articles/price_tracking_with_py_selenium/article.html#moving-to-and-interacting-with-elements",
    "title": "Price tracking with Selenium",
    "section": "Moving to and interacting with elements",
    "text": "Moving to and interacting with elements\nOnce we‚Äôve located an element, we often need to interact with it. One effective way to do this is by moving the mouse to the element‚Äôs position on the page. For this, we can use the ActionChains class, which allows us to chain together various actions:\nfrom selenium.webdriver.common.action_chains import ActionChains\n\nactions = ActionChains(driver)\nactions.move_to_element(element).perform()\nBy creating an instance of ActionChains and calling the move_to_element method, we can move the mouse cursor to the specified element. The perform method is then called to execute the action.\nUsing ActionChains is particularly useful when working with dynamic elements, such as those that reveal additional options or menus upon hover. This ensures that we can interact with elements that may not be visible or clickable until we move the mouse over them.\nAfter moving to the desired element, we can click on it by adding the click() method to the actions chain, before calling perform.\nIf we want to type something into an input field, we can use the send_keys() method. It‚Äôs a good practice to use the clear() method to clear any existing text in the input field before filling it:\ninput_field.clear()\ninput_field.send_keys(\"Your text here\")",
    "crumbs": [
      "Articles",
      "Automation",
      "Price tracking with Selenium"
    ]
  },
  {
    "objectID": "articles/price_tracking_with_py_selenium/article.html#action-functions",
    "href": "articles/price_tracking_with_py_selenium/article.html#action-functions",
    "title": "Price tracking with Selenium",
    "section": "Action functions",
    "text": "Action functions\nTo streamline our automation process, we can create two handy functions: move_click and move_click_fill. These functions will encapsulate the actions of moving to an element and performing clicks or filling input fields, making our code more organized and reusable.\ndef move_click(driver: webdriver.Chrome, by: By, element: str, wait_time: int = 10) -&gt; None:\n        wait = WebDriverWait(driver, wait_time)\n        actions = ActionChains(driver)\n        el = wait.until(EC.element_to_be_clickable((by, element)))\n        actions.move_to_element(el).click().perform()\n\ndef move_click_and_fill(driver: webdriver.Chrome, by: By, element: str, value: str, wait_time: int = 10) -&gt; None:\n        move_click(driver, by, element, wait_time)\n        el = driver.find_element(by, element)\n        el.clear()\n        el.send_keys(value)",
    "crumbs": [
      "Articles",
      "Automation",
      "Price tracking with Selenium"
    ]
  },
  {
    "objectID": "articles/price_tracking_with_py_selenium/article.html#price-tracking",
    "href": "articles/price_tracking_with_py_selenium/article.html#price-tracking",
    "title": "Price tracking with Selenium",
    "section": "Price tracking",
    "text": "Price tracking\nLet‚Äôs say you‚Äôre interested in buying the book Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. You check the price on Amazon, but it‚Äôs a bit higher than you‚Äôd like.\nWe‚Äôll first navigate to the Amazon website. Once we‚Äôre there, we‚Äôll locate the search bar at the top of the page. Next, we‚Äôll enter the title of the book, Hands-On Machine Learning, into the search bar to find the best prices available.\nWe need to instruct Selenium on how to perform these actions!\nFirst, we need to create a Selenium WebDriver instance to control the browser. We‚Äôll use our create_driver function to initialize the instance and navigate to the amazon url.\ndriver = create_driver(url=\"https://www.amazon.com\")\nWith the Amazon homepage loaded, we can now use our move_click_fill and move_click functions to enter the book‚Äôs title into the search bar e trigger the search button. After inspecting the page, we retrieved the search bar‚Äôs ID, \"twotabsearchtextbox\", and the search button ID, \"nav-search-submit-button\".\nHere‚Äôs the code to handle both actions:\nmove_click_fill(\n    driver=driver\n    , by=By.ID\n    , element=\"twotabsearchtextbox\"\n    , value=\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\"\n)\n\nmove_click(driver=driver, by=By.ID, element=\"nav-search-submit-button\")\n\nExtracting product prices\nNow that we‚Äôve successfully navigated to the search results page, we need to extract the prices for the relevant products. Here‚Äôs a more detailed breakdown of the process:\n\nInspect the HTML structure: after opening the Developer Tools and navigating to the Elements tab, take a moment to examine the HTML structure. For Amazon, products are typically organized under the class name \"s-result-item\". Upon further inspection, you‚Äôll discover that the class name for the product titles is \"a-size-medium a-color-base a-text-normal\", while the prices consist of two distinct classes: \"a-price-whole\" for the integer part and \"a-price-fraction\" for the decimal part, as shown in the images below:\n\n\n     \n\n\nExtract the title and price: we can loop through each \"s-result-item\" and extract the infos we want using the find_element method. The idea here is to organize the product titles, prices, and links into a pandas DataFrame object.\n\nimport pandas as pd\n\n# locating all titles, prices and product links\ntitles = driver.find_elements(By.CLASS_NAME, \"a-size-medium.a-color-base.a-text-normal\")\nwhole_prices = driver.find_elements(By.CLASS_NAME, \"a-price-whole\")\nfractional_prices = driver.find_elements(By.CLASS_NAME, \"a-price-fraction\")\nlinks = driver.find_elements(By.CLASS_NAME, \"a-link-normal\")\n\n# loop to append data to the DataFrame\ndata = []\n\nfor i in range(len(titles)):\n    \n    title = titles[i].text\n    price = f\"{whole_prices[i].text}.{fractional_prices[i].text}\"\n    link = links[i].get_attribute(\"href\")  # obtaining the product link\n    \n    # appending the data to the DataFrame\n    data.append({\"title\": title, \"price\": price, \"link\": link})\n\ndf = pd.DataFrame(data)\n\n# displaying the DataFrame\nprint(df)\n\n# closing the driver after collection\ndriver.quit()\nThe output would look something like this:\n\n\n\nOutput\n\n\n\n\nWhat‚Äôs next?\nOnce you have the data, the possibilities are endless:\n\nExport to excel: you can save your DataFrame as an Excel file for easy sharing or analysis;\nEmail the data: automate sending the data via email using Python‚Äôs smtplib to deliver the Excel file to your inbox;\nSchedule price checks: set up regular price checks (daily or weekly) using jobs to track changes over time;\nDeploy to the cloud: run the script on cloud platforms to automate the process remotely;\nSet price alerts: add conditions to notify you if prices drop below a certain point.\n\nDon‚Äôt forget that you can see the full notebook here!\nHappy automating! ü§ñ",
    "crumbs": [
      "Articles",
      "Automation",
      "Price tracking with Selenium"
    ]
  },
  {
    "objectID": "articles/visualizing_proportions_with_echarts4r/article.html",
    "href": "articles/visualizing_proportions_with_echarts4r/article.html",
    "title": "Visualizing proportions with echarts4r",
    "section": "",
    "text": "On this article, we‚Äôll debate some issues of representing proportion, using some features of echarts4r, which is an R package that provides an interface to the ECharts JavaScript library, making it accessible for users to create highly configurable and interactive charts directly from R.\nFor installing instructions and learning the basic syntax, check their official website (see Coene 2024b).\nFirst, we load the packages we‚Äôll be using.\nlibrary(echarts4r)\nlibrary(dplyr)",
    "crumbs": [
      "Articles",
      "Data visualization",
      "Visualizing proportions with `echarts4r`"
    ]
  },
  {
    "objectID": "articles/visualizing_proportions_with_echarts4r/article.html#pie-like-charts",
    "href": "articles/visualizing_proportions_with_echarts4r/article.html#pie-like-charts",
    "title": "Visualizing proportions with echarts4r",
    "section": "Pie-like charts",
    "text": "Pie-like charts\nThe most simple chart we can make to represent proportions is the standard pie chart. On echarts, the basic syntax goes like:\n\ndf |&gt; \n  e_chart(Group) |&gt; \n  e_pie(Total) |&gt; \n  e_theme(\"caravan\")\n\n\n\n\n\nIn this crude form, this chart may not be that interesting. In fact, it can be misleading: Notice how the minor difference between groups ‚ÄúC‚Äù and ‚ÄúD‚Äù is hardly perceived ‚Äî this is a common issue when using pie-like charts.\nThe most natural way to solve this would be to include a label, that we can do using the label argument in the e_pie function, in which you can specify the label in any form you wish via the formatter argument, that accepts any javascript function.\nHowever, this doesn‚Äôt really change the appearance of the plot. We still cannot visually perceive this difference. In this aspect, we can set the roseType = \"radius\" argument to scale the radius of each slice according to its value.\n\nformatter &lt;-\n  htmlwidgets::JS(\n    \"function(params) {\n       return params.name + ' : ' + params.value + ' (' + params.percent + '%)';\n    }\"\n  )\n\ndf |&gt; \n  e_chart(Group) |&gt; \n  e_pie(Total, roseType = \"radius\", label = list(formatter = formatter)) |&gt; \n  e_theme(\"caravan\")\n\n\n\n\n\nWith these adjustments, we can visualize the proportions more clearly, getting insights faster, while we can see more details in the label.\nWith donut charts, we‚Äôll have about the same issues. You can do it in echarts4r by passing any vector of the form c(\"55%\", \"60%\") to the radius argument in the same e_pie function:\n\n\n\n\n\n\nAs Cole Nussbaumer says in Storytelling with data1,\n\nWith pies, we are asking our audience to compare angles and areas. With a donut chart, we are asking our audience to compare one arc length to another arc length.\n\nIt may be hard for us to attribute quantitative values in two-dimensional spaces. Simply speaking, even when we can say which category has a ‚Äúbigger‚Äù value based on the size of a segment, angle, arch-lenght or area, it is hard to know by ‚Äúhow much‚Äù.\nOne approach to solve this ‚Äúhow much‚Äù issue is using horizontal bar charts. However, is worth to observe that the pie-like charts do give us a notion of the ‚Äúparts of a whole‚Äù kind of a thing ‚Äî that we‚Äôll lose using bar charts.\nWe discuss this now.",
    "crumbs": [
      "Articles",
      "Data visualization",
      "Visualizing proportions with `echarts4r`"
    ]
  },
  {
    "objectID": "articles/visualizing_proportions_with_echarts4r/article.html#bar-charts",
    "href": "articles/visualizing_proportions_with_echarts4r/article.html#bar-charts",
    "title": "Visualizing proportions with echarts4r",
    "section": "Bar charts",
    "text": "Bar charts\nFor a basic horizontal bar chart, we can use the e_bar and e_flip_coordsfunctions:\n\ndf |&gt; \n  arrange(Total) |&gt; \n  e_charts(Group) |&gt; \n  e_bar(Total, legend = FALSE, label = list(show = TRUE, position = \"right\")) |&gt; \n  e_flip_coords() |&gt; \n  e_theme(\"caravan\")\n\n\n\n\n\nObserve that now the ‚Äúhow much‚Äù issue is a matter of subtracting or completing the bars to get an intuition of the difference (or the actual difference if you subtract the values on the label üòÑ).\nHowever is not that easy to get the ‚Äúparts of a whole‚Äù intuition just by looking to the bar sizes ‚Äî we would have to mentally add their length.\nSo far, the simple example we dealt with contains only one layer, i.e, it had only the groups and the total of observations of each one. Bar charts also are very good at dealing with multi-layered data, for example: Consider this pizza dataset containing 5 pizzeria and their sales amount by flavour:\n\n\n\n\n\n\n\n\nHere, we got two categories represent: the Pizzeria and Flavour groups. Using the stacked bar chart, we can better visualize each layering. To do this in echarts4r, just pass the grouped data and explicit the stack = \"grp\" argument in the e_bar function. We can display it both vertically or horizontally:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn some cases, it might be better to unstack the bars (usually, to see some trends in data):",
    "crumbs": [
      "Articles",
      "Data visualization",
      "Visualizing proportions with `echarts4r`"
    ]
  },
  {
    "objectID": "articles/visualizing_proportions_with_echarts4r/article.html#extra-sankey-charts",
    "href": "articles/visualizing_proportions_with_echarts4r/article.html#extra-sankey-charts",
    "title": "Visualizing proportions with echarts4r",
    "section": "Extra: Sankey charts",
    "text": "Extra: Sankey charts\nWhile pie and bar charts are some fundamental tools for visualizing proportions, there are other type of visualization that I want to mention.\nSankey diagrams are typically used to visualize flows and the distribution of the quantities between different ‚Äústages‚Äù. In our context, they are particularly effective for illustrating proportions in data with multiple layers (more than two, especially).",
    "crumbs": [
      "Articles",
      "Data visualization",
      "Visualizing proportions with `echarts4r`"
    ]
  },
  {
    "objectID": "articles/visualizing_proportions_with_echarts4r/article.html#footnotes",
    "href": "articles/visualizing_proportions_with_echarts4r/article.html#footnotes",
    "title": "Visualizing proportions with echarts4r",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStorytelling with data‚Ü©Ô∏é",
    "crumbs": [
      "Articles",
      "Data visualization",
      "Visualizing proportions with `echarts4r`"
    ]
  },
  {
    "objectID": "projects/ml_cases/titanic_binary_classification/project.html",
    "href": "projects/ml_cases/titanic_binary_classification/project.html",
    "title": "Binary classification: titanic dataset",
    "section": "",
    "text": "The Titanic dataset is one of the most well-known datasets in data science publicly available on Kaggle. It has become particularly popular for binary classification tasks, allowing data scientists and enthusiasts to predict whether a passenger survived or did not survive the sinking of the ship.\nEach row in the dataset represents a passenger, with multiple features. Below is a brief overview of them:\n\n\n\n\n\n\nIn this article, we will explore how to use this data to build a step-by-step binary classification model to predict passenger survival on the Titanic.",
    "crumbs": [
      "Projects",
      "Machine learning cases",
      "Binary classification: titanic dataset"
    ]
  },
  {
    "objectID": "projects/ml_cases/titanic_binary_classification/project.html#introduction",
    "href": "projects/ml_cases/titanic_binary_classification/project.html#introduction",
    "title": "Binary classification: titanic dataset",
    "section": "",
    "text": "The Titanic dataset is one of the most well-known datasets in data science publicly available on Kaggle. It has become particularly popular for binary classification tasks, allowing data scientists and enthusiasts to predict whether a passenger survived or did not survive the sinking of the ship.\nEach row in the dataset represents a passenger, with multiple features. Below is a brief overview of them:\n\n\n\n\n\n\nIn this article, we will explore how to use this data to build a step-by-step binary classification model to predict passenger survival on the Titanic.",
    "crumbs": [
      "Projects",
      "Machine learning cases",
      "Binary classification: titanic dataset"
    ]
  },
  {
    "objectID": "projects/ml_cases/titanic_binary_classification/project.html#exploratory-data-analysis-and-data-preprocessing",
    "href": "projects/ml_cases/titanic_binary_classification/project.html#exploratory-data-analysis-and-data-preprocessing",
    "title": "Binary classification: titanic dataset",
    "section": "Exploratory Data Analysis and Data Preprocessing",
    "text": "Exploratory Data Analysis and Data Preprocessing\nIn this section, we will cover these key steps:\n\nTake a quick look at the dataset‚Äôs structure and basic statistics;\nHandle missing data;\nLook for correlations of important categorical and numerical features;\nCreate new features or modifying existing ones to improve model performance (Feature engineering);\nEncode categorical variables: convert categorical data into a numerical format for use in our ML models;\nBuild a pipeline using scikit-learn to streamline these processes.\n\nMoreover, you can find the complete code in this repository.\nFirst of all, we‚Äôll load the pandas and numpy libraries, as well as load the train data.\n\nimport pandas as pd, numpy as np\n\ntrain = pd.read_csv(\"files/dataset/train.csv\")\ntrain_copy = train.copy() # good practice\n\ntrain_copy.head()\n\n   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n0            1         0       3  ...   7.2500   NaN         S\n1            2         1       1  ...  71.2833   C85         C\n2            3         1       3  ...   7.9250   NaN         S\n3            4         1       1  ...  53.1000  C123         S\n4            5         0       3  ...   8.0500   NaN         S\n\n[5 rows x 12 columns]\n\n\nBy using the info() method, we can see that the training data contains 890 entries along with the corresponding data types of its features:\n\ntrain_copy.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\nWe can start by removing some features that may not contribute meaningfully to our predictive analysis. We‚Äôll have a look at some:\n\nTicket and PassengerId: These features do not offer meaningful insights for predicting survival, as they serve only as unique identifiers for each passenger.\nCabin: While the cabin number information might indicate passenger status, this column has a high variability and a high proportion of missing values. Given its unreliability, we choose to remove it.\n\nYou might consider the Name to be an irrelevant feature since it acts as another unique identifier for each passenger. However, upon examining the names, we can see that they include various formats and titles. By extracting the title from the passenger names, we can create a new feature called Title. Before we proceed, let‚Äôs take a look at its distribution.\n\nLet‚Äôs take the opportunity to create a generic bar plot function for categorical variables.\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef barplot(df, variable):\n    var = df[variable]\n\n    plt.figure(figsize=(12, 6))\n    sns.countplot(x=var)\n    plt.xticks(rotation=15)\n    plt.ylabel(\"Frequency\")\n\n    plt.show()\n\n\nname = train_copy.Name\ntrain_copy[\"Title\"] = [n.split(\".\")[0].split(\",\")[1].strip() for n in name]\n\nbarplot(train_copy, \"Title\")\n\n\n\n\n\n\n\n\nWe will create a function to categorize the titles into more frequent groups, to include later in our pipeline. This function will extract the titles from the passenger names and then categorize less frequent titles. Titles like Don, Dr, Major, Lady, Sir, Col, Capt, Countess, Rev, Jonkheer, and Dona will be grouped under ‚ÄúOther‚Äù. Additionally, we will standardize titles like Mlle, Ms, and Mme to Miss and Mrs, respectively.",
    "crumbs": [
      "Projects",
      "Machine learning cases",
      "Binary classification: titanic dataset"
    ]
  }
]