---
title: "Binary classification: titanic dataset"
date: "2024-10-12"
date-modified: "2024-10-14"
image: "files/image.jpg"
bibliography: files/references.bib
nocite: |
  @*
---

## Introduction

The ***Titanic dataset*** is one of the most well-known datasets in data science publicly available on [*Kaggle*](https://www.kaggle.com/c/titanic). It has become particularly popular for binary classification tasks, allowing data scientists and enthusiasts to predict whether a passenger survived or did not survive the sinking of the ship.

Each row in the dataset represents a passenger, with multiple features. Below is a brief overview of them:

```{r, echo=FALSE}

suppressMessages({
  library(dplyr)
  library(reactable)
  library(reactablefmtr)
})

titanic_features <- 
  tribble(
    ~Variable, ~Definition, ~Key,
    "PassengerId", "unique identifier for the passenger", "",
    "Survived", "Survival", "0 = No, 1 = Yes",
    "Pclass", "Ticket class", "1 = 1st, 2 = 2nd, 3 = 3rd",
    "Name", "Name of the passenger", "",
    "Sex", "Sex", "",
    "Age", "Age in years", "",
    "Sibsp", "# of siblings / spouses aboard the Titanic", "",
    "Parch", "# of parents / children aboard the Titanic", "",
    "Ticket", "Ticket number", "",
    "Fare", "Passenger fare", "",
    "Cabin", "Cabin number", "",
    "Embarked", "Port of Embarkation", "C = Cherbourg, Q = Queenstown, S = Southampton"
)

titanic_features |> 
    reactable(
       pagination = FALSE
      ,compact = TRUE
      ,outlined = FALSE
      ,bordered = FALSE
      ,sortable = TRUE
      ,resizable = TRUE
      ,showPageInfo = FALSE
      ,borderless = TRUE
      ,highlight = TRUE
      ,defaultColDef = colDef(style = list(fontSize = "13px"))
      ,theme =
        reactablefmtr::nytimes(
          header_font_size = 17
          ,header_font_color = "#262D3C"
          ,font_color = "#666666"
        )
    )
```

This dataset comes from the ***Titanic: Machine Learning from Disaster*** competition on *Kaggle*. When you download the dataset, you will receive three primary files:

-   *train.csv*, containing the training data, which includes the features of the passengers and the target variable. We will use this data to train our model;

-   *test.csv*, which, unlike in typical machine learning workflows (where the "test" set is used to evaluate the performance of the model), does not include the `Survived` column. We will use this data to generate predictions with our trained model and submit these survival outcomes to *Kaggle* for evaluation;

-   *gender_submission.csv*, which provides a sample submission format, where we must include the `PassengerId` and `Survived` columns. The values in the `Survived` column will be the predictions from our model.

Here, we will explore how to use this data to build a step-by-step binary classification model to predict passenger survival on the Titanic.

## Exploratory Data Analysis and Data Preprocessing

In this section, we will look for correlations of important categorical and numerical features; then we´ll create new features or modifying existing ones to improve model performance (*Feature engineering*); The next step will be to split our data in training and test sets, and handle missing data; Following that, we'll encode categorical variables, that is convert categorical data into a numerical format for use in our ML models; Finally, we'll build a pipeline using `scikit-learn` to streamline these processes.

Moreover, you can find the complete code in this [Jupyter notebook]().

### First analysis

First of all, we'll load the data and take a quick glance in its firsts rows and basic statistics:

```{python}
import pandas as pd, numpy as np
pd.set_option('display.expand_frame_repr', False)

data = pd.read_csv("files/dataset/train.csv")
data_copy = data.copy() # good practice

data_copy.head()
data_copy.describe()
```

#### Categorical features

We will remove some features that may not contribute meaningfully to our predictive analysis:

-   **Ticket** and **PassengerId**: these features do not offer meaningful insights for predicting survival, as they serve only as unique identifiers for each passenger.
-   **Cabin**: while the cabin number information might indicate passenger status, this column has a high variability and a high proportion of missing values. Given its unreliability, we choose to remove it.

```{python}
data_copy.drop(['Ticket', 'PassengerId', 'Cabin'], axis=1, inplace=True)
```

You might consider the **Name** to be an irrelevant feature since it acts as another unique identifier for each passenger. However, upon examining the names, we can see that they include various formats and titles. By extracting the title from the passenger names, we can create a new feature called **Title**. Before we proceed, let's take a look at its distribution.

> Let's take the opportunity to create a generic bar plot function for categorical variables.

```{python}
import matplotlib.pyplot as plt
import seaborn as sns

def barplot(df, variable):
    var = df[variable]

    plt.figure(figsize=(20, 15))
    sns.countplot(x=var)
    plt.xticks(rotation=15)
    plt.ylabel("Frequency")

    plt.show()
```

```{python}
name = data_copy.Name
data_copy["Title"] = [n.split(".")[0].split(",")[1].strip() for n in name]

barplot(data_copy, "Title")
```

We will create a function to categorize the titles into more frequent groups, to include later in our pipeline. This function will extract the titles from the passenger names and then categorize less frequent titles. Titles like *Don*, *Dr*, *Major*, *Lady*, *Sir*, *Col*, *Capt*, *Countess*, *Rev*, *Jonkheer*, and *Dona* will be grouped under "Other". Additionally, we will standardize titles like *Mlle*, *Ms*, and *Mme* to *Miss* and *Mrs*, respectively.

#### Numeric features

To a more intuitive view, we can plot the features histogram, which considers only the numerical columns. Since the `Sex` column is categorical, we’ll convert it into a numerical format, where 0 = male and 1 = female.

```{python}
data_copy['Sex'] = data_copy['Sex'].map({"male": 0, "female": 1})
```

```{python}
ax = data_copy.hist(bins=50, figsize=(12, 8))
plt.show()
```

For further investigation, we can look at the correlation between these numeric features:

```{python}
correlation = data_copy.select_dtypes(include=['float64', 'int64']).corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation, annot=True, fmt=".2f", cmap="coolwarm", cbar=True)
plt.show()
```

We can observe a strong positive relationship between `Sex` and `Survival` (0.543), and a moderate negative correlation between `Pclass` and `Survival` (-0.338), which highlights that women had a higher chance of survival, as well as those in higher classes. You may also notice the positive correlation between `Fare` and `Survival`, which is explained by the strong negative correlation between `Pclass` and `Fare`: higher fares are associated with upper classes. 

(Talvez aqui criar uma outra feature: numero de membros na família, algo assim?)

### Splitting data

With these relationships in mind, we can now move forward with splitting our data into training and testing sets, using stratified shuffle split to maintain a balanced representation.

```{python}
from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2)

for train_indices, test_indices in split.split(data, data[["Survived", "Pclass", "Sex"]]):
    strat_train_set = data.loc[train_indices]
    strat_test_set = data.loc[test_indices]
```

To ensure that the stratified shuffle split successfully preserved the balance of the features in both the training and testing sets, we examine the distribution of `Survived`, `Sex` and `Pclass` in each set:

```{python}
plt.subplot(1,2,1)
strat_train_set["Survived"].hist()
strat_train_set["Pclass"].hist()
strat_train_set["Sex"].hist()

plt.subplot(1,2,2)
strat_test_set["Survived"].hist()
strat_test_set["Pclass"].hist()
strat_test_set["Sex"].hist()

plt.show()
```

### Pipeline