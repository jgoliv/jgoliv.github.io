---
title: "WIP - Random forest classification for survival prediction on the Titanic"
date: "2024-10-12"
date-modified: today
image: "files/image.jpg"
bibliography: files/references.bib
nocite: |
  @*
---

## Introduction

The ***Titanic dataset*** is one of the most well-known datasets in data science publicly available on [*Kaggle*](https://www.kaggle.com/c/titanic). It has become particularly popular for binary classification tasks, allowing data scientists and enthusiasts to predict whether a passenger survived or not. 

Each row in the dataset represents a passenger, with multiple features. Below is a brief overview of them:

```{r, echo=FALSE}
suppressMessages({
  library(dplyr)
  library(reactable)
  library(reactablefmtr)
})

plot.reactable <- function(df, pageSize = 12) {
   df %>% 
    reactable(
      pagination = TRUE
      ,defaultPageSize = pageSize
      ,compact = TRUE
      ,outlined = FALSE
      ,bordered = FALSE
      ,sortable = TRUE
      ,resizable = TRUE
      ,showPageInfo = FALSE
      ,borderless = TRUE
      ,highlight = TRUE
      ,defaultColDef = colDef(style = list(fontSize = "13px"))
      ,theme =
        reactablefmtr::nytimes(
          header_font_size = 17
          ,header_font_color = "#262D3C"
          ,font_color = "#666666"
        )
    )
}

titanic_features <- 
  tribble(
    ~Variable, ~Definition, ~Key,
    "PassengerId", "unique identifier for the passenger", "",
    "Survived", "Survival", "0 = No, 1 = Yes",
    "Pclass", "Ticket class", "1 = 1st, 2 = 2nd, 3 = 3rd",
    "Name", "Name of the passenger", "",
    "Sex", "Sex", "",
    "Age", "Age in years", "",
    "Sibsp", "# of siblings / spouses aboard the Titanic", "",
    "Parch", "# of parents / children aboard the Titanic", "",
    "Ticket", "Ticket number", "",
    "Fare", "Passenger fare", "",
    "Cabin", "Cabin number", "",
    "Embarked", "Port of Embarkation", "C = Cherbourg, Q = Queenstown, S = Southampton"
)

plot.reactable(titanic_features)
```

\

This dataset comes from the ***Titanic: Machine Learning from Disaster*** competition on *Kaggle*. When you download it, you will receive three primary files:

-   *train.csv*, containing the training data, which includes the features of the passengers and the target variable. We will use this data to train our model;

-   *test.csv*, which, unlike in typical machine learning workflows (where the "test" set is used to evaluate the performance of the model), does not include the `Survived` column. We will use this data to generate predictions with our trained model and submit these survival outcomes to *Kaggle* for evaluation;

-   *gender_submission.csv*, which provides a sample submission format, where we must include the `PassengerId` and `Survived` columns. The values in the `Survived` column will be the predictions from our model.

Here, we will explore how to use this data to build a step-by-step binary classification model to predict passenger survival on the Titanic.

> Moreover, you can find the complete code in this [Jupyter notebook]().

## Exploratory Data Analysis and Data Preprocessing

In this section, we will:

-   Handle missing data and encode categorical variables;
-   Look for correlations of important numerical features;
-   Split our data in training and test sets;
-   Build a pipeline to streamline these processes.

First of all, we'll load the data and take a quick glance in its firsts rows and basic statistics.

```{python}
import pandas as pd
data = pd.read_csv("files/dataset/train.csv")
```

```{r, echo=FALSE}
Rdata = read.csv("files/dataset/train.csv")
plot.reactable(Rdata, pageSize = 5) 
```

Using the `info()` method, we find that the columns `Age`, `Cabin` and `Embarked` have some missing values:

```{python}
data.info()
```

For the `Age` column we can use an estimator to impute its missing values. To be more precise, we'll use the `SimpleImputer` to impute the mean.

The columns `Embarked` and `Sex` are categorical so, for a better model performance, we need to transform them into numerical features. We'll do it by using `LabelEncoder` and `OneHotEncoder`.

Using the LabelEncoder in the `Sex` column will set 0 = female and 1 = male. For the `Embarked` column, the use of OneHotEncoder's will split
the former column into four different columns: three for each value and one for missing values.

With that in mind, we now remove some features that may not contribute meaningfully to our predictive analysis:

-   **Ticket**, **PassengerId** and **Name**: these features do not offer meaningful insights for predicting survival, as they serve only as unique identifiers for each passenger.
-   **Cabin**: while the cabin number information might indicate passenger status, this column has a high variability and a high proportion of missing values. Given its unreliability, we choose to remove it.
-   **Embarked_nan**: this column represent the missing values on the Embarked column, after the use of OneHotEncoder.

Now we can make an analysis of correlation to guide our procedure of splitting the sets of training and testing. For that, we'll create a copy of our dataset and perform such changes on this copy.

::: { .panel .panel-default }
<details>
<summary>See code</summary>

```{python}
data_copy = data.copy()

# imputing the mean into the missing values of the sex column
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="mean")
data_copy["Age"] = imputer.fit_transform(data_copy[["Age"]])

# encoding categorical features
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

label_encoder = LabelEncoder()
data_copy["Sex"] = label_encoder.fit_transform(data_copy["Sex"])

one_hot_encoder = OneHotEncoder()
matrix = one_hot_encoder.fit_transform(data_copy[["Embarked"]]).toarray()
df = pd.DataFrame(matrix, columns=one_hot_encoder.get_feature_names_out(["Embarked"]))
df.index = data_copy.index
data_copy = pd.concat([data_copy, df], axis=1)

# dropping features
data_copy.drop(["PassengerId", "Embarked", "Name", "Ticket", "Cabin", "Embarked_nan"], axis=1, errors="ignore", inplace=True)

data_copy.head()
```
</details> 
:::

For a more intuitive view, we can plot those features histogram...

```{python, echo=FALSE}
import matplotlib.pyplot as plt

ax = data_copy.hist(bins=50, figsize=(12, 10))
plt.show()
```

...and, for further investigation, we can look at the correlation between them:

```{python, echo=FALSE}
import seaborn as sns

correlation = data_copy.select_dtypes(include=['float64', 'int64']).corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation, annot=True, fmt=".2f", cmap="coolwarm", cbar=True)
plt.show()
```

We can observe a strong positive relationship between `Sex` and `Survival` (0.543), and a moderate negative correlation between `Pclass` and `Survival` (-0.338), which highlights that women had a higher chance of survival, as well as those in higher classes. 

You may also notice the positive correlation between `Fare` and `Survival`, which is explained by the strong negative correlation between `Pclass` and `Fare`: higher fares are associated with upper classes.

With these relationships in mind, we can now move forward with splitting our data into training and testing sets, using stratified shuffle split to maintain a balanced representation.

```{python}
from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2)

for train_indices, test_indices in split.split(data, data[["Survived", "Pclass", "Sex"]]):
    strat_train_set = data.loc[train_indices]
    strat_test_set = data.loc[test_indices]
```

To streamline the processes described previously, we can now build a pipeline.

::: { .panel .panel-default }
<details>
<summary>See code</summary>

```{python}
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline

class AgeImputer(BaseEstimator, TransformerMixin):
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        imputer = SimpleImputer(strategy="mean")
        X["Age"] = imputer.fit_transform(X[["Age"]])
        return X

class FeatureEncoder(BaseEstimator, TransformerMixin):
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        label_encoder = LabelEncoder()
        one_hot_encoder = OneHotEncoder()

        X["Sex"] = label_encoder.fit_transform(X["Sex"])

        matrix = one_hot_encoder.fit_transform(X[["Embarked"]]).toarray()
        df = pd.DataFrame(matrix, columns=one_hot_encoder.get_feature_names_out(["Embarked"]))

        df.index = X.index
        X = pd.concat([X, df], axis=1)
        
        return X 

class FeatureDropper(BaseEstimator, TransformerMixin):
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        return X.drop(["Embarked", "Name", "Ticket", "Cabin", "Embarked_nan"], axis=1, errors="ignore")
      
pipeline = Pipeline([
    ("age_imputer", AgeImputer()),
    ("feature_encoder", FeatureEncoder()),
    ("feature_dropper", FeatureDropper())
])
```

</details> 
:::

After applying the `pipeline` to the training set, we move on to a final preprocessing step: standardization. Here, we'll use `StandardScaler`:

```{python}
from sklearn.preprocessing import StandardScaler

strat_train_set = pipeline.fit_transform(strat_train_set)

X = strat_train_set.drop(['Survived'], axis=1)
y = strat_train_set['Survived']

scaler = StandardScaler()
X_data = scaler.fit_transform(X)
y_data = y.to_numpy()
```

After standardization, `X_data` contains the scaled features, while `y_data` stores the target values as a NumPy array, making both compatible with Scikit-Learn models.

## Training the model
